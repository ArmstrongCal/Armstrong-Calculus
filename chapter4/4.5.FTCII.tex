\section{The Fundamental Theorem of Calculus, part II} \label{S:4.4.FTC}

\begin{goals}
\item How can we find the exact value of a definite integral without taking the limit of a Riemann sum?
\item What is the statement of the Fundamental Theorem of Calculus Part II, and how do antiderivatives of functions play a key role in applying the theorem?
\item What is the meaning of the definite integral of a rate of change in contexts other than when the rate of change represents velocity?
\item What is an indefinite integral and how is its notation used in discussing antiderivatives?
\item How do the First and Second Fundamental Theorems of Calculus enable us to formally see how differentiation and integration are almost inverse processes?
\end{goals}

%------------------------------------
% SUBSECTION INTRODUCTION
%------------------------------------
\subsection*{Introduction}

\begin{marginfigure}[5in] % MARGIN FIGURE
\margingraphics{figs/4/4-5_VelF.pdf}
\caption{A velocity function.} \label{fig:4-5_VelF}
\end{marginfigure}

Much of our work in Chapter~\ref{CH:4} has been motivated by the velocity-distance problem:  if we know the instantaneous velocity function, $v(t)$, for a moving object on a given time interval $[a,b]$, can we determine its exact distance traveled on $[a,b]$?  In the vast majority of our discussion in Sections~\ref{S:4.1.VelocityDistance}-\ref{S:4.3.DefiniteIntegral}, we have focused on the fact that this distance traveled is connected to the area bounded by $y = v(t)$ and the $t$-axis on $[a,b]$.  In particular, for any nonnegative velocity function $y = v(t)$ on $[a,b]$, we know that the exact area bounded by the velocity curve and the $t$-axis on the interval tells us the total distance traveled, which is also the value of the definite integral $\int_a^b v(t) \, dt$.  In the situation where velocity is sometimes negative, the total area bounded by the velocity function still tells us distance traveled, while the net signed area that the function bounds tells us the object's change in position.  Recall, for instance, the introduction to Section~\ref{S:4.2.Riemann}, where we observed that for the velocity function in Figure~\ref{fig:4-5_VelF}, the total distance $D$ traveled by the moving object on $[a,b]$ is 
\[ D = A_1 + A_2 + A_3, \]
while the total change in the object's position on $[a,b]$ is 
\[ s(b) - s(a) = A_1 - A_2 + A_3. \]
While the areas $A_1$, $A_2$, and $A_3$, which are each given by definite integrals, may be computed through limits of Riemann sums (and in select special circumstances through familiar geometric formulas), in the present section we turn our attention to an alternate approach, similar to the one we encountered in Activity~\ref{A:4.1.2}.  To explore these ideas further, we consider the following preview activity.

\input{previews/4.5.PA1} % PREVIEW ACTIVITY

%-------------------------------------------------------------------------------
% SUBSECTION THE FUNDAMENTAL THEOREM OF CALCULUS, PART II
%-------------------------------------------------------------------------------
\subsection*{The Fundamental Theorem of Calculus, Part II} \index{fundamental theorem of calculus}

\begin{marginfigure}[2in] % MARGIN FIGURE
\margingraphics{figs/4/4-5_VelV.pdf}
\caption{Finding the distance traveled when we know an object's velocity function $v$.} \label{fig:4-5_VelV}
\end{marginfigure}

Consider the setting where we know the position function $s(t)$ of an object moving along an axis, as well as its corresponding velocity function $v(t)$, and for the moment let us assume that $v(t)$ is positive on $[a,b]$.  Then, as shown in Figure~\ref{fig:4-5_VelV}, we know two different perspectives on the distance, $D$, the object travels: one is that $D = s(b) - s(a)$, which is the object's change in position.  The other is that the distance traveled is the area under the velocity curve, which is given by the definite integral, so $D = \int_a^b v(t) \, dt$.

Of course, since both of these expressions tell us the distance traveled, it follows that they are equal, so
\begin{equation} \label{E:FTCVel}
s(b) - s(a) = \int_a^b v(t) \, dt.
\end{equation}
Furthermore, we know that Equation~(\ref{E:FTCVel}) holds even when velocity is sometimes negative, since $s(b) - s(a)$ is the object's change in position over $[a,b]$, which is simultaneously measured by the total net signed area on $[a,b]$ given by $\int_a^b v(t) \, dt$.

Perhaps the most powerful part of Equation~(\ref{E:FTCVel}) lies in the fact that we can compute the integral's value if we can find a formula for $s$.  Remember, $s$ and $v$ are related by the fact that $v$ is the derivative of $s$, or equivalently that $s$ is an \emph{antiderivative} of $v$.  For example, if we have an object whose velocity is $v(t) = 3t^2 + 40$ feet per second (which is always nonnegative), and wish to know the distance traveled on the interval $[1,5]$, we have that
\begin{eqnarray*}
D & = & \int_1^5 v(t) \,dt \\
& = & \int_1^5 (3t^2 + 40) \, dt \\
& = & s(5) - s(1),
\end{eqnarray*}
where $s$ is an antiderivative of $v$.  We know that the derivative of $t^3$ is $3t^2$ and that the derivative of $40t$ is $40$, so it follows that if $s(t) = t^3 + 40t$, then $s$ is a function whose derivative is $v(t) = s'(t) = 3t^2 + 40$, and thus we have found an antiderivative of $v$.  Therefore,
\begin{eqnarray*}
D & = & \int_1^5 3t^2 + 40 \, dt \\
& = & s(5) - s(1) \\
& = & (5^3 + 40 \cdot 5) - (1^3 + 40\cdot 1) \\
& = & 284 \ \mbox{feet}.
\end{eqnarray*} 

\begin{marginfigure}[2in] % MARGIN FIGURE
\margingraphics{figs/4/4-5_VelC.pdf}
\caption{The exact area of the region enclosed by $v(t) = 3t^2 + 40$ on $[1,5]$.} \label{fig:4-5_VelC}
\end{marginfigure}

Note the key lesson of this example:  to find the distance traveled, we needed to compute the area under a curve, which is given by the definite integral.  But to evaluate the integral, we found an antiderivative, $s$, of the velocity function, and then computed the total change in $s$ on the interval.  In particular, observe that we have found the exact area of the region shown in Figure~\ref{fig:4-5_VelC}, and done so without a familiar formula (such as those for the area of a triangle or circle) and without directly computing the limit of a Riemann sum. As we proceed to thinking about contexts other than just velocity and position, it turns out to be advantageous to have a shorthand symbol for a function's antiderivative.  In the general setting of a continuous function $f$, we will often denote an antiderivative of $f$ by $F$, so that the relationship between $F$ and $f$ is that $F'(x) = f(x)$ for all relevant $x$.  Using the notation $V$ in place of $s$ (so that $V$ is an antiderivative of $v$) in Equation~(\ref{E:FTCVel}), we find it is equivalent to write that
\begin{equation} \label{E:FTCV} % EQUATION
V(b) - V(a) = \int_a^b v(t) \, dt.
\end{equation}
 Now, in the general setting of wanting to evaluate the definite integral $\int_a^b f(x) \, dx$ for an arbitrary continuous function $f$, we could certainly think of $f$ as representing the velocity of some moving object, and $x$ as the variable that represents time.  And again, Equations~(\ref{E:FTCVel}) and~(\ref{E:FTCV}) hold for any continuous velocity function, even when $v$ is sometimes negative.   This leads us to see that Equation~(\ref{E:FTCV}) tells us something even more important than the change in position of a moving object: it offers a shortcut route to evaluating any definite integral, provided that we can find an antiderivative of the integrand.  The second part of the Fundamental Theorem of Calculus (FTCII) \index{FTCII} summarizes these observations.
 
\concept{The Fundamental Theorem of Calculus, part II} % CONCEPT
{If $f$ is a continuous function on $[a,b]$, and $F$ is any antiderivative of $f$, then
\[ \int_a^b f(x) \ dx = F(b) - F(a). \]
} % end concept

\proof We can use the first part of the Fundamental Theorem to prove the second part.  In the first part, we showed that the integral, or area, function 
\[ A(x) = \int_a^x f(t) \ dt \quad \mbox{for} \quad a \leq x \leq b \]
is an antiderivative of the function $f$.  We have also shown by the Mean Value Theorem that if two functions have the same derivative, then they must differ by only a constant.  In other words, if both $A$ and $F$ are antiderivatives of $f$, then $\ds F(x) = A(x) + C$, where $C$ is an arbitrary constant.

Recall that $A(a) = 0$, which implies that $F(a) = A(a) + C = C$.  Notice also that $F(b) = A(b) + C$, and we have
\begin{eqnarray*}
F(b) - F(a) & = & [A(b) + C] - [A(a) + C] \\
& = & A(b) - A(a) \\
& = & A(b).
\end{eqnarray*}
And what is $A(b)$ with respect to the area function?  $\ds A(b) = \int_a^b f(t) \ dt$, which is the same as $\ds \int_a^b f(x) \ dx$. So
\begin{eqnarray*}
F(b) - F(a) & = & \int_a^b f(x) \ dx.
\end{eqnarray*}
\qed

A common alternate notation for $F(b) - F(a)$ is 
\[ F(b) - F(a) = F(x) \Big|_a^b \]
where we read the righthand side as ``the function $F$ evaluated from $a$ to $b$.''  In this notation, the FTCII says that
\[ \int_a^b f(x) \ dx = F(x) \Big|_a^b. \]

The FTCII opens the door to evaluating exactly a wide range of integrals.  In particular, if we are interested in a definite integral for which we can find an antiderivative $F$ for the integrand $f$, then we can evaluate the integral exactly.  For instance since $\frac{d}{dx}[\frac{1}{3}x^3] = x^2$, the FTCII tells us that
\begin{eqnarray*}
\int_0^1 x^2 \, dx & = & \left. \frac{1}{3} \ x^3 \right|_0^1 \\
& = & \frac{1}{3} \ (1)^3 - \frac{1}{3} \ (0)^3 \\
& = & \frac{1}{3}.
\end{eqnarray*}

\vspace*{-.25cm}

\input{examples/4-5_Eg1} % EXAMPLE

\input{examples/4-5_Eg2} % EXAMPLE

But finding an antiderivative can be far from simple; in fact, often finding a formula for an antiderivative is very hard or even impossible.  While we can differentiate just about any function, even some relatively simple ones don't have an elementary antiderivative.  A significant portion of integral calculus (which is the main focus of second semester college calculus) is devoted to understanding the problem of finding antiderivatives.

%---------------------------------------------
% SUBSECTION BASIC ANTIDERIVATIVES
%---------------------------------------------
\subsection*{Basic antiderivatives}

The general problem of finding an antiderivative is difficult.  In part, this is due to the fact that we are trying to undo the process of differentiating, and the undoing is much more difficult than the doing.  For example, while it is evident that an antiderivative of $f(x) = \sin(x)$ is $F(x) = -\cos(x)$ and that an antiderivative of $g(x) = x^2$ is $G(x) = \frac{1}{3} x^3$, combinations of $f$ and $g$ can be far more complicated.  Consider such functions as
\[ 5\sin(x) - 4x^2, \enskip x^2 \sin(x), \enskip \frac{\sin(x)}{x^2}, \enskip \mbox{and} \ \sin(x^2). \]
What is involved in trying to find an antiderivative for each?  From our experience with derivative rules, we know that while derivatives of sums and constant multiples of  basic functions are simple to execute, derivatives involving products, quotients, and composites of familiar functions are much more complicated.  Thus, it stands to reason that antidifferentiating products, quotients, and composites of basic functions may be even more challenging.  We defer our study of all but the most elementary antiderivatives to later in the text.

We do note that each time we have a function for which we know its derivative, we have a \emph{function-derivative pair}, which also leads us to knowing the antiderivative of a function.  For instance, since we know that 
\[ \frac{d}{dx}[-\cos(x)] = \sin(x), \]
it follows that $F(x) = -\cos(x)$ is an antiderivative of $f(x) = \sin(x)$.  It is equivalent to say that $f(x) = \sin(x)$ is the derivative of $F(x) = -\cos(x)$, and thus $F$ and $f$ together form the function-derivative pair.  Clearly, every basic derivative rule leads us to such a pair, and thus to a known antiderivative.   In Activity~\ref{A:4.5.1}, we will construct a list of most of the basic antiderivatives we know at this time.  Furthermore, those rules will enable us to antidifferentiate sums and constant multiples of basic functions.  For example, if $f(x) = 5\sin(x) - 4x^2$, note that since $-\cos(x)$ is an antiderivative of $\sin(x)$ and $\frac{1}{3}x^3$ is an antiderivative of $x^2$, it follows that 
\[ F(x) = -5\cos(x) - 4\cdot \frac{1}{3}x^3 \]
is an antiderivative of $f$, by the sum and constant multiple rules for differentiation.

\input{activities/4.5.Act1} % ACTIVITY

We now revisit the fact that each function has more than one antiderivative.  Because the derivative of any constant is zero, any time we seek an arbitrary antiderivative, we may add a constant of our choice.  For instance, if we want to determine an antiderivative of $g(x) = x^2$, we know that $G(x) = \frac{1}{3}x^3$ is one such function.  But we could alternately have chosen $G(x) = \frac{1}{3}x^3 + 7$, since in this case as well, $G'(x) = x^2$.  In some contexts later on in calculus, it is important to discuss the most general antiderivative of a function.  If $g(x) = x^2$, we say that the \emph{general antiderivative} \index{antiderivative!general} or \emph{indefinite integral} of $g$ is 
\[ G(x) = \frac{1}{3}x^3 + C, \]
where $C$ represents an arbitrary real number constant.  Regardless of the formula for $g$, including $+C$ in the formula for its antiderivative $G$ results in the most general possible antiderivative.

\definition{General Antiderivatives/Indefinite Integrals} % DEFINITION
{Let a function $f(x)$ be given. A \textbf{general antiderivative} or \textbf{indefinite integral} of $f(x)$ is the infinite family of  functions 
\[ F(x) + C\]
where $C$ is an arbitrary constant such that 
\[ F'(x) = f(x).\]
}% end definition

Given a function $f$ and one of its antiderivatives $F$, we know \textit{all} antiderivatives of $f$ have the form $F(x) + C$ for some constant $C$. Using the above definition, we can say that
\[ \int f(x) \ dx = F(x) + C. \]
Let's analyze this indefinite integral notation.\index{integration!notation}

\begin{figure}[h]
\begin{center}
\includegraphics[scale=1.375]{figs/4/4-5_figanti1.pdf}
\caption{Understanding the indefinite integral notation.}\label{fig:4-5_figanti1}
\end{center}
\end{figure}

\input{examples/4-5_Eg3} % EXAMPLE

This final step of ``verifying our answer'' is important both practically and theoretically. In general, taking derivatives is easier than finding antiderivatives so checking our work is easy and vital as we learn.  We also see that taking the derivative of our answer returns the function in the integrand. Thus we can say that: 
\[ \frac{d}{dx}\left(\int f(x)\ dx\right) = f(x). \]
Differentiation ``undoes'' the work done by antidifferentiation. 

%--------------------------------------------------
% SUBSECTION INITIAL VALUE PROBLEMS
%--------------------------------------------------
\subsection*{Initial Value Problems}

We have seen that the derivative of a position function gave a velocity function, and the derivative of a velocity function describes the acceleration.\index{initial value problem} We can now go ``the other way:'' the antiderivative of an acceleration function gives a velocity function, etc. While there is just one derivative of a given function, there are infinite antiderivatives. Therefore we cannot ask ``What is \textit{the} velocity of an object whose acceleration is $-32$ft/s$^2$?'', since there is more than one answer. 

We can find \textit{the} answer if we provide more information with the question, as done in the following example.

\input{examples/4-5_Eg4} % EXAMPLE

\input{examples/4-5_Eg5} % EXAMPLE

\input{activities/4.5.Act2} % ACTIVITY

%-------------------------------------------------------------------------------------------
% SUBSECTION DIFFERENTIATING AN INTEGRAL FUNCTION
%-------------------------------------------------------------------------------------------

\subsection*{Differentiating an Integral Function}

We have seen that the Fundamental Theorem of Calculus (FTC) enables us to construct an antiderivative $F$ of any continuous function $f$ by defining $A$ by the corresponding integral function $A(x) = \int_c^x f(t) \, dt$.  Said differently, if we have a function of the form $A(x) = \int_c^x f(t) \, dt$, then we know that $A'(x) = \frac{d}{dx} \left[\int_c^x f(t) \, dt \right] = f(x)$.  This shows that integral functions, while perhaps having the most complicated formulas of any functions we have encountered, are nonetheless particularly simple to differentiate.  For instance, if 
\[ A(x) = \int_{\pi}^x \sin(t^2) \ dt, \]
then by the FTC, we know immediately that
\[ A'(x) = \sin(x^2). \]

Restating this result more generally for an arbitrary function $f$, we know by the FTC that
\[ \frac{d}{dx} \left[ \int_a^x f(t) \, dt \right] = f(x). \]
In words, the last equation essentially says that ``the derivative of the integral function whose integrand is $f$, is $f$.''  In this sense, we see that if we first integrate the function $f$ from $t = a$ to $t = x$, and then differentiate with respect to $x$, these two processes ``undo'' one another.

Taking a different approach, say we begin with a function $f(t)$ and differentiate with respect to $t$.  What happens if we follow this by integrating the result from $t = a$ to $t = x$?  That is, what can we say about the quantity
\[ \int_a^x \frac{d}{dt} \left[ f(t) \right] \, dt? \]
Here, we use the First FTC and note that $f(t)$ is an antiderivative of $\frac{d}{dt} \left[ f(t) \right].$  Applying this result and evaluating the antiderivative function, we see that
\begin{eqnarray*}
\int_a^x \frac{d}{dt} \left[ f(t) \right] \, dt & = & f(t) \bigg\vert_a^x \\
							& = & f(x) - f(a).
\end{eqnarray*} 
Thus, we see that if we apply the processes of first differentiating $f$ and then integrating the result from $a$ to $x$, we return to the function $f$, minus the constant value $f(a)$.  So in this situation, the two processes almost undo one another, up to the constant $f(a)$.

The observations made in the preceding two paragraphs demonstrate that differentiating and integrating (where we integrate from a constant up to a variable) are almost inverse processes.  In one sense, this should not be surprising:  integrating involves antidifferentiating, which reverses the process of differentiating.  On the other hand, we see that there is some subtlety involved, as integrating the derivative of a function does not quite produce the function itself.  This is connected to a key fact we observed, which is that any function has an entire family of antiderivatives, and any two of those antiderivatives differ only by a constant.

\newpage

%-------------
% SUMMARY
%-------------
\begin{summary}
\item We can find the exact value of a definite integral without taking the limit of a Riemann sum or using a familiar area formula by finding the antiderivative of the integrand, and hence applying the Fundamental Theorem of Calculus.
\item The Fundamental Theorem of Calculus says that if $f$ is a continuous function on $[a,b]$ and $F$ is an antiderivative of $f$, then
\[ \int_a^b f(x) \, dx = F(b) - F(a). \]
Hence, if we can find an antiderivative for the integrand $f$, evaluating the definite integral comes from simply computing the change in $F$ on $[a,b]$. 
\item A slightly different perspective on the FTC allows us to restate it as the Total Change Theorem, which says that
\[ \int_a^b f'(x) \, dx = f(b) - f(a), \]
for any continuously differentiable function $f$.   This means that the definite integral of the instantaneous rate of change of a function $f$ on an interval $[a,b]$ is equal to the total change in the function $f$ on $[a,b]$.
\end{summary}

\clearpage

%--------------
% EXERCISES
%--------------
\input{exercises/4-5.FTC2(Ex)} 

\cleardoublepage